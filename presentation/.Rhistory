plot_data %>%
# group_by(condition) %>%
group_by(condition, stim_type, grammaticality, attractor) %>%
summarise_all(n = n(),
mean_RT = sprintf("%.2f", mean(RT)),
stdev_RT = sprintf("%.2f", sd(RT)),
mean_logRT =sprintf("%.2f",  mean(log(RT))),
stdev_logRT = sprintf("%.2f", sd(log(RT)))
) %>%
kable(align = c('l','l','l','l','c','c','c','c','c'))
plot_data <- total_data %>%
filter(word == "was" | word == "were") %>%
filter(!is.na(stim_type))
plot_data %>%
# group_by(condition) %>%
group_by(condition, stim_type, grammaticality, attractor) %>%
summarize(n = n(),
mean_RT = sprintf("%.2f", mean(RT)),
stdev_RT = sprintf("%.2f", sd(RT)),
mean_logRT =sprintf("%.2f",  mean(log(RT))),
stdev_logRT = sprintf("%.2f", sd(log(RT)))
) %>%
kable(align = c('l','l','l','l','c','c','c','c','c'))
plot_data %>%
# group_by(condition) %>%
group_by(condition, stim_type, grammaticality, attractor) %>%
summarise_at(n = n(),
mean_RT = sprintf("%.2f", mean(RT)),
stdev_RT = sprintf("%.2f", sd(RT)),
mean_logRT =sprintf("%.2f",  mean(log(RT))),
stdev_logRT = sprintf("%.2f", sd(log(RT)))
) %>%
kable(align = c('l','l','l','l','c','c','c','c','c'))
plot_data %>%
# group_by(condition) %>%
group_by(condition, stim_type, grammaticality, attractor) %>%
summarize(n = n(),
mean_RT = sprintf("%.2f", mean(RT)),
stdev_RT = sprintf("%.2f", sd(RT)),
mean_logRT =sprintf("%.2f",  mean(log(RT))),
stdev_logRT = sprintf("%.2f", sd(log(RT)))
) %>%
kable(align = c('l','l','l','l','c','c','c','c','c'))
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, cache = FALSE)
library(dplyr)
library(dplyr)
library(ggplot2)
library(dplyr)
library(ggplot2)
library(knitr)
library(png)
install.packages(png)
install.packages("png")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, cache = FALSE)
library(dplyr)
library(ggplot2)
library(knitr)
library(png)
library(grid)
# ggplot theme
theme_set(theme_bw())
img <- readPNG("./fig4.png")
grid.raster(img)
img <- readPNG("./fig5.png")
grid.raster(img)
library(imager)
install.packages("imager")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, cache = FALSE)
library(dplyr)
library(ggplot2)
library(knitr)
library(imager)
# ggplot theme
theme_set(theme_bw())
filepath <- "./results.csv"
data <- read.csv(filepath, header=F, comment.char="#", col.names=paste0("V", 1:12), stringsAsFactors=F, na.strings = "NULL")
names(data)[names(data)=="V1"] <- "system_time"
names(data)[names(data)=="V2"] <- "trial"
names(data)[names(data)=="V3"] <- "controller"
names(data)[names(data)=="V4"] <- "item_num"
names(data)[names(data)=="V5"] <- "element_num"
# setting aside form and participant data
form_data <- data %>% filter(controller == "Form")
raw_data <- data %>% filter(controller == "Question" | controller == "DashedSentence")
names(raw_data)[names(raw_data)=="V6"] <- "condition"
# renaming conditions to be more human-readable
raw_data[raw_data$condition == "f", "condition"] <- "filler"
raw_data[raw_data$condition == "test_grammatical_singular", "condition"] <- "grammatical_sg"
raw_data[raw_data$condition == "test_grammatical_plural", "condition"] <- "grammatical_pl"
raw_data[raw_data$condition == "test_ungrammatical_singular", "condition"] <- "ungrammatical_sg"
raw_data[raw_data$condition == "test_ungrammatical_plural", "condition"] <- "ungrammatical_pl"
## NON-FORM DATA: first pass at formatting data
# adding stim_type column to easily categorize target and filler data
raw_data <- raw_data %>%
mutate(stim_type = ifelse(condition == "filler",
"filler",
ifelse(condition %in% c("grammatical_sg", "grammatical_pl",
"ungrammatical_sg", "ungrammatical_pl"),
"target",
NA
)
))
# adding columns for grammaticality and attractor number (sg vs. pl) for categorization
raw_data <- raw_data %>%
mutate(grammaticality = ifelse(condition %in% c("grammatical_sg", "grammatical_pl"),
"grammatical",
ifelse(condition %in% c("ungrammatical_sg", "ungrammatical_pl"),
"ungrammatical",
NA)
)
) %>%
mutate(attractor = ifelse(condition %in% c("grammatical_sg", "ungrammatical_sg"),
"singular",
ifelse(condition %in% c("grammatical_pl", "ungrammatical_pl"),
"plural",
NA)
))
## QUESTION DATA
# formatting column names for two types of data
# first type: question
question_data <- raw_data %>% filter(controller == "Question" & condition != "practice")
names(question_data)[names(question_data)=="V8"] <- "question"
names(question_data)[names(question_data)=="V9"] <- "answer"
names(question_data)[names(question_data)=="V10"] <- "accuracy"
names(question_data)[names(question_data)=="V11"] <- "response_time"
# fixing data types
question_data <- question_data %>%
mutate(accuracy = as.integer(accuracy)) %>%
mutate(response_time = as.integer(response_time))
# adding columns for human-readable (string) and numerical (for calculations) accuracy
question_data <- question_data %>%
mutate(accuracy_numeric = as.integer(accuracy)) %>%
mutate(accuracy_label = ifelse(accuracy == 1, "correct", "false"))
## READING DATA
# second type: dashed sentence
reading_data <- raw_data %>% filter(controller == "DashedSentence" & condition != "practice")
names(reading_data)[names(reading_data)=="V8"] <- "word_num"
names(reading_data)[names(reading_data)=="V9"] <- "word"
names(reading_data)[names(reading_data)=="V10"] <- "RT"
names(reading_data)[names(reading_data)=="V11"] <- "newline"
names(reading_data)[names(reading_data)=="V12"] <- "sentence"
# fixing data types
reading_data <- reading_data %>%
mutate(RT = as.integer(RT))
# joining the question and reading dataframes
# each row now marked for accuracy
# skipping duplicated columns (V1-thru-V7 except for merge pivots)
# using all columns from question data
question_columns <- question_data[, -3] # all cols but controller (since we're merging data with two different values)
# using non-duplicate columns for reading data (+ pivot columns)
reading_columns <- reading_data[, c("trial", "item_num", "word_num", "word", "RT", "newline", "sentence")]
total_data <- merge(question_columns, reading_columns)
img <- load.image("./fig4.png")
thumb <- resize(img, 200,100)
plot(thumb)
thumb <- resize(img, 300,200)
plot(thumb)
thumb <- resize(img, 1000)
plot(thumb)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, cache = FALSE)
library(dplyr)
library(ggplot2)
library(knitr)
library(imager)
# ggplot theme
theme_set(theme_bw())
filepath <- "./results.csv"
data <- read.csv(filepath, header=F, comment.char="#", col.names=paste0("V", 1:12), stringsAsFactors=F, na.strings = "NULL")
names(data)[names(data)=="V1"] <- "system_time"
names(data)[names(data)=="V2"] <- "trial"
names(data)[names(data)=="V3"] <- "controller"
names(data)[names(data)=="V4"] <- "item_num"
names(data)[names(data)=="V5"] <- "element_num"
# setting aside form and participant data
form_data <- data %>% filter(controller == "Form")
raw_data <- data %>% filter(controller == "Question" | controller == "DashedSentence")
names(raw_data)[names(raw_data)=="V6"] <- "condition"
# renaming conditions to be more human-readable
raw_data[raw_data$condition == "f", "condition"] <- "filler"
raw_data[raw_data$condition == "test_grammatical_singular", "condition"] <- "grammatical_sg"
raw_data[raw_data$condition == "test_grammatical_plural", "condition"] <- "grammatical_pl"
raw_data[raw_data$condition == "test_ungrammatical_singular", "condition"] <- "ungrammatical_sg"
raw_data[raw_data$condition == "test_ungrammatical_plural", "condition"] <- "ungrammatical_pl"
## NON-FORM DATA: first pass at formatting data
# adding stim_type column to easily categorize target and filler data
raw_data <- raw_data %>%
mutate(stim_type = ifelse(condition == "filler",
"filler",
ifelse(condition %in% c("grammatical_sg", "grammatical_pl",
"ungrammatical_sg", "ungrammatical_pl"),
"target",
NA
)
))
# adding columns for grammaticality and attractor number (sg vs. pl) for categorization
raw_data <- raw_data %>%
mutate(grammaticality = ifelse(condition %in% c("grammatical_sg", "grammatical_pl"),
"grammatical",
ifelse(condition %in% c("ungrammatical_sg", "ungrammatical_pl"),
"ungrammatical",
NA)
)
) %>%
mutate(attractor = ifelse(condition %in% c("grammatical_sg", "ungrammatical_sg"),
"singular",
ifelse(condition %in% c("grammatical_pl", "ungrammatical_pl"),
"plural",
NA)
))
## QUESTION DATA
# formatting column names for two types of data
# first type: question
question_data <- raw_data %>% filter(controller == "Question" & condition != "practice")
names(question_data)[names(question_data)=="V8"] <- "question"
names(question_data)[names(question_data)=="V9"] <- "answer"
names(question_data)[names(question_data)=="V10"] <- "accuracy"
names(question_data)[names(question_data)=="V11"] <- "response_time"
# fixing data types
question_data <- question_data %>%
mutate(accuracy = as.integer(accuracy)) %>%
mutate(response_time = as.integer(response_time))
# adding columns for human-readable (string) and numerical (for calculations) accuracy
question_data <- question_data %>%
mutate(accuracy_numeric = as.integer(accuracy)) %>%
mutate(accuracy_label = ifelse(accuracy == 1, "correct", "false"))
## READING DATA
# second type: dashed sentence
reading_data <- raw_data %>% filter(controller == "DashedSentence" & condition != "practice")
names(reading_data)[names(reading_data)=="V8"] <- "word_num"
names(reading_data)[names(reading_data)=="V9"] <- "word"
names(reading_data)[names(reading_data)=="V10"] <- "RT"
names(reading_data)[names(reading_data)=="V11"] <- "newline"
names(reading_data)[names(reading_data)=="V12"] <- "sentence"
# fixing data types
reading_data <- reading_data %>%
mutate(RT = as.integer(RT))
# joining the question and reading dataframes
# each row now marked for accuracy
# skipping duplicated columns (V1-thru-V7 except for merge pivots)
# using all columns from question data
question_columns <- question_data[, -3] # all cols but controller (since we're merging data with two different values)
# using non-duplicate columns for reading data (+ pivot columns)
reading_columns <- reading_data[, c("trial", "item_num", "word_num", "word", "RT", "newline", "sentence")]
total_data <- merge(question_columns, reading_columns)
knitr::include_graphics("./fig4.png")
knitr::include_graphics("./fig5.png")
knitr::include_graphics("./fig6.png")
plot_data <- question_data
plot_data %>%
# group_by(condition) %>%
group_by(condition, stim_type, grammaticality, attractor) %>%
summarize(n = n(),
n_correct = sum(accuracy_numeric),
accuracy = sprintf("%.2f%%", 100 * mean(accuracy_numeric)),
stdev = sprintf("%.4f", sd(accuracy_numeric)),
) %>%
kable(align = c('l','l','l','l','c','c','c','c'))
plot_data <- question_data
ggplot(plot_data, aes(x="", y=accuracy_numeric)) +
geom_violin(aes(fill = stim_type)) +
ylab("adjective choice accuracy") +
xlab("stimuli type") +
facet_grid(~stim_type, switch = 'x') +
ggtitle("Adjective choice accuracy in fillers vs. targets") +
theme(legend.position="none")
plot_data <- question_data
ggplot(plot_data, aes(x="", y=accuracy_numeric)) +
geom_violin(aes(fill = stim_type)) +
# geom_boxplot() +
# stat_summary(fun=mean, geom="point", shape=2, size=2) +
ylab("adjective choice accuracy") +
xlab("condition") +
facet_grid(~condition, switch = 'x') +
ggtitle("Adjective choice accuracy by condition") +
theme(legend.position="none")
plot_data <- question_data %>%
filter(stim_type == "target")
ggplot(plot_data, aes(x="", y=accuracy_numeric)) +
geom_violin(aes(fill = condition)) +
# geom_boxplot() +
# stat_summary(fun=mean, geom="point", shape=2, size=2) +
ylab("adjective choice accuracy") +
xlab("condition") +
facet_grid(attractor~grammaticality, switch = 'x') +
ggtitle("Adjective choice accuracy by condition")
plot_data <- question_data %>%
filter(stim_type == "target") %>%
mutate(head_matching = as.factor(ifelse(accuracy_label == "correct", "head-matching", "attractor-matching")))
ggplot(plot_data, aes(x=head_matching, fill=head_matching)) +
geom_bar(color="black") +
facet_grid(attractor~grammaticality) +
theme(legend.position="none") +
xlab("adjective choice")
# facet_grid(attractor~grammaticality, switch = 'x') +
# ggtitle("Adjective choice accuracy by condition")
plot_data <- total_data %>%
filter(word == "was" | word == "were") %>%
filter(!is.na(stim_type))
plot_data %>%
# group_by(condition) %>%
group_by(condition, stim_type, grammaticality, attractor) %>%
summarize(n = n(),
mean_RT = sprintf("%.2f", mean(RT)),
stdev_RT = sprintf("%.2f", sd(RT)),
mean_logRT =sprintf("%.2f",  mean(log(RT))),
stdev_logRT = sprintf("%.2f", sd(log(RT)))
) %>%
kable(align = c('l','l','l','l','c','c','c','c','c'))
plot_data <- total_data %>%
filter(word == "was" | word == "were") %>%
filter(!is.na(stim_type))%>%
mutate(log_RT = log(RT))
ggplot(plot_data, aes(x="", y=log_RT)) +
geom_violin(aes(fill=stim_type)) +
# stat_summary(fun=mean, geom="point", shape=2, size=2) +
ylab("verb log(RT)") +
xlab("condition") +
facet_grid(~condition, switch = 'x') +
ggtitle("Verb Reading Time by condition")
plot_data <- reading_data %>%
filter(stim_type == "target") %>%
filter(word_num < 9) %>% # cuts 1 word after verb bc. sentences are of different lengths
mutate(log_RT = log(RT)) %>%
group_by(word_num, condition) %>%
# mutate(mean_RT = mean(RT)) %>%
# mutate(stdev_RT = sd(RT)) %>%
# mutate(lo_RT = mean_RT - stdev_RT) %>%
# mutate(hi_RT = mean_RT + stdev_RT) %>%
mutate(mean_log_RT = mean(log_RT)) %>%
mutate(stdev_log_RT = sd(log_RT)) %>%
mutate(lo_log_RT = mean_log_RT - stdev_log_RT) %>%
mutate(hi_log_RT = mean_log_RT + stdev_log_RT) %>%
ungroup()
## overall RTs
plot_data$word_num <- as.integer(plot_data$word_num)
ggplot(plot_data, aes(x=word_num, y=mean_log_RT, color=condition)) +
geom_line() +
geom_point(size=2) +
# geom_errorbar(aes(ymin = lo_log_RT, ymax = hi_log_RT)) +
# geom_jitter()
scale_x_continuous(breaks = seq(0,9)) +
ylab("mean log(RT)") +
ggtitle("Word-by-word mean Reading Times")
plot_data <- reading_data %>%
filter(stim_type == "target") %>%
filter(word_num < 9) %>% # cuts 1 word after verb bc. sentences are of different lengths
mutate(log_RT = log(RT)) %>%
group_by(word_num, condition) %>%
# mutate(mean_RT = mean(RT)) %>%
# mutate(stdev_RT = sd(RT)) %>%
# mutate(lo_RT = mean_RT - stdev_RT) %>%
# mutate(hi_RT = mean_RT + stdev_RT) %>%
mutate(mean_log_RT = mean(log_RT)) %>%
mutate(stdev_log_RT = sd(log_RT)) %>%
mutate(lo_log_RT = mean_log_RT - stdev_log_RT) %>%
mutate(hi_log_RT = mean_log_RT + stdev_log_RT) %>%
ungroup() %>%
mutate(word_num = as.integer(word_num))
ggplot(plot_data, aes(x=word_num, y=mean_log_RT, color=condition)) +
geom_line() +
geom_point(size=2) +
geom_errorbar(aes(ymin = lo_log_RT, ymax = hi_log_RT)) +
# geom_jitter()
scale_x_continuous(breaks = seq(0,9)) +
ylab("mean log(RT)") +
ggtitle("Word-by-word Reading Time")
plot_data <- reading_data %>%
filter(stim_type == "target") %>%
filter(word_num < 9) %>% # cuts 1 word after verb bc. sentences are of different lengths
mutate(log_RT = log(RT)) %>%
group_by(word_num, grammaticality, attractor) %>%
# mutate(mean_RT = mean(RT)) %>%
# mutate(stdev_RT = sd(RT)) %>%
# mutate(lo_RT = mean_RT - stdev_RT) %>%
# mutate(hi_RT = mean_RT + stdev_RT) %>%
mutate(mean_log_RT = mean(log_RT)) %>%
mutate(stdev_log_RT = sd(log_RT)) %>%
mutate(lo_log_RT = mean_log_RT - stdev_log_RT) %>%
mutate(hi_log_RT = mean_log_RT + stdev_log_RT) %>%
ungroup() %>%
mutate(word_num = as.integer(word_num))
## overall RTs
plot_data$word_num <- as.integer(plot_data$word_num)
ggplot(plot_data, aes(x=word_num, y=mean_log_RT, color=condition)) +
geom_line() +
geom_point(size=2) +
geom_errorbar(aes(ymin = lo_log_RT, ymax = hi_log_RT)) +
# geom_jitter()
scale_x_continuous(breaks = seq(0,9)) +
facet_grid(attractor~grammaticality, scales = "free_y") +
ggtitle("Word-by-word mean reading times")
plot_data <- total_data %>%
filter(stim_type == "target") %>%
filter(word_num < 9) %>% # cuts 1 word after verb bc. sentences are of different lengths
mutate(log_RT = log(RT)) %>%
group_by(word_num, grammaticality, attractor, accuracy) %>%
# mutate(mean_RT = mean(RT)) %>%
# mutate(stdev_RT = sd(RT)) %>%
# mutate(lo_RT = mean_RT - stdev_RT) %>%
# mutate(hi_RT = mean_RT + stdev_RT) %>%
mutate(mean_log_RT = mean(log_RT)) %>%
mutate(stdev_log_RT = sd(log_RT)) %>%
mutate(lo_log_RT = mean_log_RT - stdev_log_RT) %>%
mutate(hi_log_RT = mean_log_RT + stdev_log_RT) %>%
ungroup() %>%
mutate(word_num = as.integer(word_num))
## RTs by accuracy
plot_data$word_num <- as.integer(plot_data$word_num)
ggplot(plot_data, aes(x=word_num, y=mean_log_RT, color=accuracy_label)) +
geom_line() +
geom_point(size=2) +
# geom_errorbar(aes(ymin = lo_log_RT, ymax = hi_log_RT)) +
# geom_jitter()
scale_x_continuous(breaks = seq(0,9)) +
facet_grid(attractor~grammaticality, scales = "free_y") +
ggtitle("Word-by-word mean reading times")
plot_data <- total_data %>%
filter(stim_type == "target") %>%
filter(word_num < 9) %>% # cuts 1 word after verb bc. sentences are of different lengths
mutate(log_RT = log(RT)) %>%
group_by(word_num, grammaticality, attractor, accuracy) %>%
# mutate(mean_RT = mean(RT)) %>%
# mutate(stdev_RT = sd(RT)) %>%
# mutate(lo_RT = mean_RT - stdev_RT) %>%
# mutate(hi_RT = mean_RT + stdev_RT) %>%
mutate(mean_log_RT = mean(log_RT)) %>%
mutate(stdev_log_RT = sd(log_RT)) %>%
mutate(lo_log_RT = mean_log_RT - stdev_log_RT) %>%
mutate(hi_log_RT = mean_log_RT + stdev_log_RT) %>%
ungroup() %>%
mutate(word_num = as.integer(word_num))
## RTs by accuracy
plot_data$word_num <- as.integer(plot_data$word_num)
ggplot(plot_data, aes(x=word_num, y=mean_log_RT, color=accuracy_label)) +
geom_line() +
geom_point(size=2) +
geom_errorbar(aes(ymin = lo_log_RT, ymax = hi_log_RT)) +
# geom_jitter()
scale_x_continuous(breaks = seq(0,9)) +
facet_grid(attractor~grammaticality, scales = "free_y") +
ggtitle("Word-by-word mean reading times")
ggplot(plot_data, aes(x=word_num, y=mean(RT), color=condition)) +
geom_line() +
geom_point(size=2) +
# geom_errorbar(aes(ymin = lo_log_RT, ymax = hi_log_RT)) +
# geom_jitter()
scale_x_continuous(breaks = seq(0,9)) +
ylab("mean log(RT)") +
ggtitle("Word-by-word mean Reading Times")
plot_data <- reading_data %>%
filter(stim_type == "target") %>%
filter(word_num < 9) %>% # cuts 1 word after verb bc. sentences are of different lengths
mutate(log_RT = log(RT)) %>%
group_by(word_num, condition) %>%
# mutate(mean_RT = mean(RT)) %>%
# mutate(stdev_RT = sd(RT)) %>%
# mutate(lo_RT = mean_RT - stdev_RT) %>%
# mutate(hi_RT = mean_RT + stdev_RT) %>%
mutate(mean_log_RT = mean(log_RT)) %>%
mutate(stdev_log_RT = sd(log_RT)) %>%
mutate(lo_log_RT = mean_log_RT - stdev_log_RT) %>%
mutate(hi_log_RT = mean_log_RT + stdev_log_RT) %>%
ungroup()
plot_data <- reading_data %>%
filter(stim_type == "target") %>%
filter(word_num < 9) %>% # cuts 1 word after verb bc. sentences are of different lengths
mutate(log_RT = log(RT)) %>%
group_by(word_num, condition) %>%
# mutate(mean_RT = mean(RT)) %>%
# mutate(stdev_RT = sd(RT)) %>%
# mutate(lo_RT = mean_RT - stdev_RT) %>%
# mutate(hi_RT = mean_RT + stdev_RT) %>%
mutate(mean_log_RT = mean(log_RT)) %>%
mutate(stdev_log_RT = sd(log_RT)) %>%
mutate(lo_log_RT = mean_log_RT - stdev_log_RT) %>%
mutate(hi_log_RT = mean_log_RT + stdev_log_RT) %>%
ungroup()
## overall RTs
plot_data$word_num <- as.integer(plot_data$word_num)
ggplot(plot_data, aes(x=word_num, y=mean(RT), color=condition)) +
geom_line() +
geom_point(size=2) +
# geom_errorbar(aes(ymin = lo_log_RT, ymax = hi_log_RT)) +
# geom_jitter()
scale_x_continuous(breaks = seq(0,9)) +
ylab("mean log(RT)") +
ggtitle("Word-by-word mean Reading Times")
plot_data <- total_data %>%
filter(stim_type == "target") %>%
filter(word_num < 8) %>% # cuts 1 word after verb bc. sentences are of different lengths
mutate(log_RT = log(RT)) %>%
group_by(word_num, grammaticality, attractor, accuracy) %>%
# mutate(mean_RT = mean(RT)) %>%
# mutate(stdev_RT = sd(RT)) %>%
# mutate(lo_RT = mean_RT - stdev_RT) %>%
# mutate(hi_RT = mean_RT + stdev_RT) %>%
mutate(mean_log_RT = mean(log_RT)) %>%
mutate(stdev_log_RT = sd(log_RT)) %>%
mutate(lo_log_RT = mean_log_RT - stdev_log_RT) %>%
mutate(hi_log_RT = mean_log_RT + stdev_log_RT) %>%
ungroup() %>%
mutate(word_num = as.integer(word_num))
## RTs by accuracy
plot_data$word_num <- as.integer(plot_data$word_num)
ggplot(plot_data, aes(x=word_num, y=mean_log_RT, color=accuracy_label)) +
geom_line() +
geom_point(size=2) +
geom_errorbar(aes(ymin = lo_log_RT, ymax = hi_log_RT)) +
# geom_jitter()
scale_x_continuous(breaks = seq(0,9)) +
facet_grid(attractor~grammaticality, scales = "free_y") +
ggtitle("Word-by-word mean reading times")
